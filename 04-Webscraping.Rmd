# Webscraping, iterations, and functions

*The third assignment had students scraping the Center for Snow and Avalanche Studies website for snow data in two different basins using a function and a for loop. Students were then asked to analyze different parameters.*

*Composed by Matt Ross*

```{r setup, include=FALSE}
library(rvest)
library(tidyverse)
library(lubridate)
library(readxl)
library(ggthemes)
library(knitr)
library(kableExtra)

```


## Simple web scraping

R can read html using either rvest, xml, or xml2 packages. Here we are going to navigate to the Center for Snow and Avalanche Studies  [Website](https://snowstudies.org/archived-data/) and read a table in. This table contains links to data we want to programatically download for three sites. We don't know much about these sites, but they contain incredibly rich snow, temperature, and precip data. 


### Reading an html 

#### Extract CSV links from webpage

```{r}
site_url <- 'https://snowstudies.org/archived-data/'

#Read the web url
webpage <- read_html(site_url)

#See if we can extract tables and get the data that way
tables <- webpage %>%
  html_nodes('table') %>%
  magrittr::extract2(3) %>%
  html_table(fill = TRUE)
#That didn't work, so let's try a different approach

#Extract only weblinks and then the URLs!
links <- webpage %>%
  html_nodes('a') %>%
  .[grepl('24hr',.)] %>%
  html_attr('href')

```

### Data Download

#### Download data in a for loop

```{r}

splits <- str_split_fixed(links,'/',8)

dataset <- splits[,8] 

file_names <- paste0('C:/Users/13074/Documents/ESS580/3_snow_functions_iteration/data',dataset)

for(i in 1:3){
  download.file(links[i],destfile=file_names[i])
}

downloaded <- file.exists(file_names)

evaluate <- !all(downloaded)

```


#### Download data in a map

```{r}

if(evaluate == T){
  map2(links[1:3],file_names[1:3],download.file)
}else{print('data already downloaded')}

```

### Data read-in 

#### Read in just the snow data as a loop

```{r}

snow_files <- file_names %>%
  .[!grepl('SG_24',.)] %>%
  .[!grepl('PTSP',.)]

#empty_data <- list()

# snow_data <- for(i in 1:length(snow_files)){
#   empty_data[[i]] <- read_csv(snow_files[i]) %>%
#     select(Year,DOY,Sno_Height_M)
# }

#snow_data_full <- do.call('rbind',empty_data)

#summary(snow_data_full)
```


#### Read in the data as a map function

```{r}

our_snow_reader <- function(file){
  name = str_split_fixed(file,'/',7)[,7] %>%
    gsub('_24hr.csv','',.) %>% 
    gsub('data', '',.)
  df <- read_csv(file) %>%
    select(Year,DOY,Sno_Height_M) %>%
    mutate(site = name)
}

snow_data_full <- map_dfr(snow_files,our_snow_reader)

#summary(snow_data_full)
```


#### Plot snow data

```{r snow plot}
snow_yearly <- snow_data_full %>%
  group_by(Year,site) %>%
  summarize(mean_height = mean(Sno_Height_M,na.rm=T))

ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + 
  geom_point() +
  ggthemes::theme_few() + 
  ggthemes::scale_color_few()
```


## Assignment

### Question 1: 
Extract the meteorological data URLs. Here we want you to use the `rvest` package to get the URLs for the `SASP forcing` and `SBSP_forcing` meteorological datasets.


```{r, warning=F,message=F}

site_url <- 'https://snowstudies.org/archived-data/'

#Read the web url
webpage <- read_html(site_url)

#Extract only weblinks and then the URLs!
links_met <- webpage %>%
  html_nodes('a') %>%
  .[grepl('forcing',.)] %>%
  html_attr('href')

```


### Question 2: 
Download the meteorological data. Use the `download_file` and `str_split_fixed` commands to download the data and save it in your data folder. You can use a for loop or a map function. 

```{r, warning=F,message=F}

#Grab only the name of the file by splitting out on forward slashes
splits_met <- str_split_fixed(links_met,'/',8)

#Keep only the 8th column
dataset_met <- splits_met[,8] 

#generate a file list for where the data goes
file_names_met <- paste0('C:/Users/13074/Documents/ESS580/3_snow_functions_iteration/data',dataset_met)


#This for loop takes the links_met list and adds a destination to it.
for(i in 1:2){
  download.file(links_met[i],destfile=file_names_met[i])
}

downloaded_met <- file.exists(file_names_met)

```

### Question 3: 
Write a custom function to read in the data and append a site column to the data. 

```{r, warning=F,message=F}

# this code grabs the variable names from the metadata pdf file
library(pdftools)
headers <- pdf_text('https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf') %>%
  readr::read_lines(.) %>%
  trimws(.) %>%
  str_split_fixed(.,'\\.',2) %>%
  .[,2] %>%
  .[1:26] %>%
  str_trim(side = "left")

#headers

file_names_met[]

file <- file_names_met[1]

# Reads in the data
our_met_reader <- function(file){
  name = str_split_fixed(file,'_',5)[,5] %>% 
    gsub('SP_Forcing_Data.txt','',.)
    df <- read_fwf(file) %>%
      setNames(headers) %>% 
    mutate(siteID =name)

}

```

### Question 4: 
Use the `map` function to read in both meteorological files. Display a summary of your tibble.

```{r, warning=F,message=F}

file_names_met

#using the map function to create a df of both meteorological files
met_data_full <- map_dfr(file_names_met,our_met_reader)

#displaing that df
head(met_data_full) %>% 
  kable(.,'html') %>%
  kable_styling() %>%
  scroll_box(width='800px',height='300px')

```


### Question 5:
Make a line plot of mean temp by year by site (using the `air temp [K]` variable). Is there anything suspicious in the plot? Adjust your filtering if needed.


*Some of the sensors seemed to be offline during 2003-2004 which was affecting the temperature sensor. These years were filtered out.* *Air temperature across both Senator Beck and Swamp Angel sites show increases in temperature during the period of record. Similarity in temperature patterns are likely due to the sites' proximity to each other.*

```{r temp year by site, warning=F,message=F}

#filtering for year, site and mean air temp
mean_a_t_k <- met_data_full %>%
  rename(air_temp_K = "air temp [K]") %>%
  filter(year > 2004) %>% 
  group_by(year,siteID) %>%
  summarize(mean_a_t_k = mean(air_temp_K,na.rm=T))

ggplot(mean_a_t_k,aes(x=year,y=mean_a_t_k,color=siteID)) + 
  geom_line(size = 1) +
  theme_base() + 
  scale_color_few() +
  geom_smooth(method=lm) +
  labs(x='Year', y="Air Temperature [K]")
  
```



### Question 6: 
Write a function that makes line plots of monthly average temperature at each site for a given year. Use a for loop to make these plots for 2005 to 2010. Are monthly average temperatures at the Senator Beck Study Plot ever warmer than the Snow Angel Study Plot?
Hint: https://ggplot2.tidyverse.org/reference/print.ggplot.html

*Both Swamp Angel Senator Beck sites demonstrate temperature fluctuation typical of high elevation locations in the northern hemisphere. Similarity in temperature patterns are likely due to the sites' proximity to each other.*

```{r function, warning=F,message=F}

#a function for temp by month and site by year, with a for loop to create plots by year:
plot_figure_k <- function(df, year) {
mean_month_a_t_k <- df %>%
  rename(air_temp_K = "air temp [K]") %>%
  mutate(month = month(month, label = FALSE)) %>% 
  filter(yr == year) %>% 
  group_by(year,month,siteID) %>%
  summarize(mean_a_t_k = mean(air_temp_K,na.rm=T)) 
  
figure_k <-
 ggplot(mean_month_a_t_k,aes(x=month,y=mean_a_t_k,color=siteID)) + 
  geom_line(size = 1) +
  theme_base() + 
  scale_color_few() +
  labs(x='Month', y="Air Temp [K]") +
  facet_wrap(~year)
  
print(figure_k)
}

years <- c(2005:2011)

for (yr in years){
  plot_figure_k(met_data_full, year)
}

```

## Bonus

### Bonus 1:
Make a plot of average daily precipitation by day of year (averaged across all available years). Color each site. 

*Site specific precipitation was unavailable, so the precipitation shown is for both Senator Beck and Swamp Angel study sites. Average precipitation follows a typical annual cycle for the Southern Rocky Mountains, with higher seasonal precipitation in the late summer.* 

```{r average precip, warning=F,message=F}

# changing the date to a recognizable form to day of year, normalizing by area and summarizing by averaging precip
precip <- met_data_full %>% 
  mutate(date = make_date(year, month, day)) %>% 
  mutate(date = as.Date(date))%>% 
  mutate(doy = yday(date)) %>% 
  rename(precip_unit_area = "precip [kg m-2 s-1]") %>%
  mutate(precip_mm = precip_unit_area*86400) %>% 
  group_by(doy) %>% 
  summarise(average_precip = mean(precip_mm))

ggplot(precip,aes(x=doy,y=average_precip)) + 
  geom_point(size = 2, color="blue") +
  theme_base() + 
  labs(x='Day of Year', y="Average Precipitation (mm)")

```


### Bonus 2: 
Use a function and for loop to create yearly plots of precipitation by day of year. Color each site. 

*Site specific precipitation was unavailable, so the precipitation shown is for both Senator Beck and Swamp Angel study sites. Precipitation for 2005-2011 follows a typical annual cycle for the Southern Rocky Mountains, with higher seasonal precipitation in mid to late summer.*

```{r precip year, warning=F,message=F}
#a function to change date to day of year, normalized by area and grouped by year, with a for loop to create plots by year:
plot_figure_prcp <- function(df, year) {
precip_doy <- df %>%
  mutate(date = make_date(year, month, day)) %>% 
  mutate(date = as.Date(date))%>% 
  mutate(doy = yday(date)) %>%
  filter(yr == year) %>% 
  rename(precip_unit_area = "precip [kg m-2 s-1]") %>%
  mutate(precip_mm = precip_unit_area*86400) %>% 
  group_by(doy, year)

figure_prcp <-
 ggplot(precip_doy,aes(x=doy,y=precip_mm)) + 
  geom_line(color="blue") +
  theme_base() + 
  scale_color_few() +
  labs(x='Day of Year', y="Precipitation (mm)") +
  facet_wrap(~year)

print(figure_prcp)
}

years <- c(2005:2011)

for (yr in years){
  plot_figure_prcp(met_data_full, year)
}

```
